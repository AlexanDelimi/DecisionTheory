import pandas
import re
import numpy
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import f1_score, precision_score, recall_score
from sklearn.model_selection import train_test_split
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import time
import tensorflow
from keras.callbacks import EarlyStopping

start_time = time.time()    # Start the time.

ps = PorterStemmer()    # Initialize the stemmer.
tf_idf = TfidfVectorizer()  # Initialize tf-idf.
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)   # Initialize early stopping.
stop_words = set(stopwords.words('english'))    # Set language for stop words.

filen = pandas.read_csv("./SocialMedia_Negative.csv")
filep = pandas.read_csv("./SocialMedia_Positive.csv")
file = pandas.concat([filen, filep], axis=0, ignore_index=True)
text = file.Text
labels = file.Sentiment

for i,label in enumerate(labels):
    if label == 'negative':
        labels[i] = 0.0
    else:
        labels[i] = 1.0




vector_text = text.to_numpy()
# print(vector_text, "\n\n")
vectors_of_words = []
for strings in range(len(vector_text)):     # Enter each sentence of vector_text.
    # print(vector_text[strings])
    vector_text[strings] = re.sub(r'\w+:\/{2}[\d\w-]+(\.[\d\w-]+)*(?:(?:\/[^\s/]*))*', '', vector_text[strings], flags=re.MULTILINE)
    vector_text[strings] = re.sub("[^a-zA-Z0-9 ]", "",vector_text[strings])
    vector_text[strings] = vector_text[strings].lower()
    for word in word_tokenize(vector_text[strings]):    # Enter each word of each sentence.
        # print(word)
        new_word = ps.stem(word)
        vector_text[strings] = vector_text[strings].replace(word, new_word)
        if new_word in stop_words:  # Stop word checking.
            vector_text[strings] = vector_text[strings].replace(word, "")
    vector_text[strings] = re.sub(' +',' ',vector_text[strings])
    # print(vector_text[strings])

# print("\n\n\n\n\n\n\n\n\n\n\n")
# print(vector_text, "\n\n")

x = tf_idf.fit(vector_text)
# print(x.vocabulary_)
#print(tf_idf.get_feature_names())

x = tf_idf.transform(vector_text)   # Executes the tf-idf transformation.

# # print(x.shape)
# print(x)
# # print(x.toarray())

df = pandas.DataFrame(x.toarray(), columns=tf_idf.get_feature_names())
print("\n\n\n")
# # pandas.set_option('display.max_columns', None)
# # print(len(df))
# # print(len(label))
df.insert(len(df.columns), "labelz", labels, True)   # Inserting dataframe label in dataframe df.
# # pandas.set_option('display.max_rows', None)
# print(df, "\n\n")
# print("Size of dataframe: \t\t", df.size, "\n\n")

X_train, X_test, y_train, y_test = train_test_split(df.loc[:, df.columns != 'labelz'], df.labelz, test_size=0.25)   # Split to train and test.

# print(X_train)

# X_train = tensorflow.keras.utils.normalize(X_train, axis=0)
# X_test = tensorflow.keras.utils.normalize(X_test, axis=0)
# y_train = tensorflow.keras.utils.normalize(y_train, axis=0)

num_y_train = y_train.to_numpy()
num_y_test = y_test.to_numpy()
num_x_test = X_test.to_numpy()
num_x_train = X_train.to_numpy()

model = tensorflow.keras.models.Sequential()    # Model initialization.
model.add(tensorflow.keras.layers.Flatten())    # Flatten model. -> 1-D array like.
model.add(tensorflow.keras.layers.Dense(64, activation=tensorflow.nn.relu))    # Add layer.
model.add(tensorflow.keras.layers.Dense(64, activation=tensorflow.nn.relu))    # Add layer.
model.add(tensorflow.keras.layers.Dense(2, activation=tensorflow.nn.softmax))   # Number of possible answers are 2 (0 || 1).
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])   # Model compilation.
model.fit(num_x_train, num_y_train, epochs=2, batch_size=16, verbose=1, validation_data=(num_x_test, num_y_test), callbacks=[es])  # Model fit.

val_loss, val_acc = model.evaluate(num_x_test, num_y_test )  # Model evaluation.
print("\n\nLoss: \t\t", val_loss, "\nAccuracy: \t", val_acc, "\n\n")


predictions = model.predict(num_x_test)  # Model predictions.
# # print(predictions)

predictions_list = list()
for k in range(len(predictions)):   # For each prediction.
    predictions_list.append(numpy.argmax(predictions[k]))   # Append prediction to predictions_list.

